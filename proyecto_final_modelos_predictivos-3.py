# -*- coding: utf-8 -*-
"""Proyecto Final - Modelos Predictivos.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UoKw0f9OJM8Vvq1bRSg2vxMfPE7mbQAQ

Cargué el archivo .csv e hice una visualizacion de las filas del dataframe

# EDA - Analisis exploratorio de los Datos
"""

import pandas as pd

# Cargar el archivo CSV
file_path = 'monkeypox.csv'
data = pd.read_csv(file_path)

# Mostrar las primeras filas del dataframe para entender su estructura
data.head(), data.info()

"""Ver los valores nulos por columna"""

# Convertir la columna 'date' a formato datetime
data['date'] = pd.to_datetime(data['date'])

# Verificar si hay valores nulos en el dataset
missing_values = data.isnull().sum()

# Mostrar los valores nulos por columna
missing_values

"""Elimine la columna iso code ya que tenia la mayor cantidad de datos nulos y otras columnas relacionadas como new cases, deaths, etc...."""

# Eliminar la columna 'iso_code'
data_cleaned = data.drop(columns=['iso_code'])

# Eliminar filas donde las columnas con valores nulos tengan 0
columns_with_nans = ['total_cases', 'total_deaths', 'new_cases', 'new_deaths',
                     'new_cases_smoothed', 'new_deaths_smoothed',
                     'new_cases_per_million', 'total_cases_per_million',
                     'new_cases_smoothed_per_million', 'new_deaths_per_million',
                     'total_deaths_per_million', 'new_deaths_smoothed_per_million']

# Mantener filas donde las columnas mencionadas no tienen nulos
data_cleaned = data_cleaned.dropna(subset=columns_with_nans)

# Verificar el nuevo número de filas y columnas
data_cleaned.shape

"""Genere un nuevo archivo limpio."""

# Guardar el DataFrame limpio en un nuevo archivo CSV
output_path = 'cleaned_monkeypox.csv'
data_cleaned.to_csv(output_path, index=False)

output_path

# Resumen estadístico de las columnas numéricas
data_cleaned.describe()

"""Procesar los datos para el modelo Holt-Winters y realizar la prueba de raiz unitaria para verificar la estacionariedad. Nos damos cuenta que el valor de p sugiere que la serie no es estacionaria."""

from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.holtwinters import ExponentialSmoothing
from statsmodels.tsa.stattools import adfuller

# Agrupar los datos por fecha y sumar los casos nuevos para manejar duplicados
data_daily = data_cleaned.groupby('date')['new_cases'].sum().reset_index()
data_daily.set_index('date', inplace=True)
data_daily = data_daily.asfreq('D').fillna(0)  # Asegurar frecuencia diaria

# Decomposición de la serie temporal
result = seasonal_decompose(data_daily['new_cases'], model='additive', extrapolate_trend='freq')

# Modelo de Holt-Winters
model_hw = ExponentialSmoothing(data_daily['new_cases'],
                                trend='add', seasonal='add',
                                seasonal_periods=365).fit()

# Predicción para los últimos 30 días en el dataset
prediction_hw = model_hw.forecast(30)

# Prueba de Dickey-Fuller Aumentada para verificar estacionariedad
adf_result = adfuller(data_daily['new_cases'])

result, prediction_hw.head(), adf_result

# Asumiendo que ya tienes cargado el DataFrame 'data_global' con los datos globales
# La columna 'new_cases' contiene los casos diarios

# Calcular la desviación estándar de los casos globales
std_new_cases = data_global['new_cases'].std()

# Mostrar el resultado
print(f"La desviación estándar de los casos globales es: {std_new_cases}")

"""Aplico una diferenciacion de primer orden y los resultados de Dickey-Fuller muestran que ahora la serie es estacionaria, lo que me permitira modelar con el metodo ARIMA."""

# Diferenciación de la serie temporal para hacerla estacionaria
data_daily_diff = data_daily['new_cases'].diff().dropna()  # Diferencia de primer orden

# Revisión de la serie temporal diferenciada
result_diff = seasonal_decompose(data_daily_diff, model='additive', extrapolate_trend='freq')

# Nueva prueba de Dickey-Fuller Aumentada para verificar estacionariedad
adf_result_diff = adfuller(data_daily_diff)

result_diff, adf_result_diff

"""La diferenciación ha ayudado a estabilizar la media, haciendo la serie más adecuada para el análisis con modelos de series temporales como ARIMA.

Observamos cómo las fluctuaciones y la varianza se mantienen más consistentes a lo largo del tiempo, lo cual es un buen indicativo para proceder con el modelado.
"""

import matplotlib.pyplot as plt

# Visualizar la serie temporal diferenciada
plt.figure(figsize=(14, 6))
plt.plot(data_daily_diff, label='Diferencia de Primer Orden', color='blue')
plt.title('Serie Temporal Diferenciada de Nuevos Casos de Viruela del Mono')
plt.xlabel('Fecha')
plt.ylabel('Diferencia de Nuevos Casos')
plt.legend()
plt.grid(True)
plt.show()

# Graficar la serie temporal de nuevos casos de viruela del mono a nivel global
plt.figure(figsize=(10,6))
plt.plot(data_global.index, data_global['new_cases'], label='Nuevos Casos Globales', color='b')
plt.title('Serie Temporal de Nuevos Casos de Viruela del Mono a Nivel Global')
plt.xlabel('Fecha')
plt.ylabel('Nuevos Casos')
plt.grid(True)
plt.legend()
plt.show()

data_global['new_cases'].hist(bins=30)
plt.title('Distribución de Nuevos Casos')
plt.show()

"""# ARIMA"""



from statsmodels.tsa.arima.model import ARIMA

# Ajustar el modelo ARIMA con p=1, d=1, q=1
model = ARIMA(data_global['new_cases'], order=(1, 1, 1))
fitted_model = model.fit()

# Mostrar resumen del modelo
print(fitted_model.summary())

# Realizar predicciones para los próximos 30 días
predictions = fitted_model.forecast(steps=30)
print(predictions)

# Cargar el archivo CSV
data = pd.read_csv('cleaned_monkeypox.csv')

# Trabajamos con los datos globales
data_global = data.copy()

# Mostrar las primeras filas para asegurarnos de que se cargaron los datos correctamente
print(data_global.head())

# Ajustar el modelo ARIMA sin diferenciación
model = ARIMA(data_global['new_cases'], order=(1, 1, 1))
fitted_model = model.fit()

# Predicciones
predictions = fitted_model.forecast(steps=30)
print(predictions)

import statsmodels.api as sm

# Asumiendo que 'data_global' ya está preparada y es estacionaria
fig, ax = plt.subplots(2, 1, figsize=(10, 8))
fig = sm.graphics.tsa.plot_acf(data_global['new_cases'], lags=40, ax=ax[0])
fig = sm.graphics.tsa.plot_pacf(data_global['new_cases'], lags=40, ax=ax[1])
plt.show()

# Ajustar el modelo ARIMA con p=1, d=1, q=1
model = ARIMA(data_global['new_cases'], order=(1, 1, 1))
fitted_model = model.fit()

# Mostrar resumen del modelo
print(fitted_model.summary())

# Realizar predicciones para los próximos 30 días
predictions = fitted_model.forecast(steps=30)
print(predictions)

plt.plot(test.index, test, label='Valores Reales')
plt.plot(arima_predictions.index, arima_predictions, label='Predicciones ARIMA')
plt.legend()
plt.show()

from sklearn.metrics import mean_squared_error

# Divide los datos en entrenamiento y prueba
train = data_global['new_cases'][:-30]
test = data_global['new_cases'][-30:]

# Ajustar el modelo ARIMA en el conjunto de entrenamiento
model = ARIMA(train, order=(1, 1, 1))
fitted_model = model.fit()

# Realizar predicciones para el conjunto de prueba
predictions = fitted_model.forecast(steps=len(test))

# Calcular el MAPE
def mape_adjusted(y_true, y_pred):
    mask = y_true != 0  # Filtrar valores donde y_true no es 0
    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100

mape_result_adjusted = mape_adjusted(test, predictions)
print("MAPE ajustado:", mape_result_adjusted)

# Calcular el RMSE
rmse = np.sqrt(mean_squared_error(test, predictions))
print("RMSE:", rmse)

def mape_handle_zeros(y_true, y_pred):
    y_true = np.where(y_true == 0, 1e-5, y_true)  # Reemplazar ceros con un valor pequeño
    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100

mape_result_handle_zeros = mape_handle_zeros(test, predictions)
print("MAPE con manejo de ceros:", mape_result_handle_zeros)

print("Valores reales (test):", test)
print("\nPredicciones:", predictions)

error_percent = np.abs((test - predictions) / (test + 1e-5)) * 100  # Añadir valor pequeño para evitar división por cero
print(error_percent)  # Revisa el porcentaje de error en cada punto

mae_result = mean_absolute_error(test, predictions)
print("MAE:", mae_result)

"""# HOLT - WINTERS"""

from statsmodels.tsa.holtwinters import ExponentialSmoothing

# Cargar y preparar los datos globales
data_global = data[['date', 'new_cases']].copy()

# Convertir la columna de fechas al formato datetime
data_global['date'] = pd.to_datetime(data_global['date'])

# Eliminar duplicados basados en la columna 'date', sumando los casos duplicados (puedes ajustarlo según tus necesidades)
data_global = data_global.groupby('date').sum().reset_index()

# Establecer la columna de fechas como índice
data_global.set_index('date', inplace=True)

# Asegurarse de que los datos estén en frecuencia diaria y rellenar los valores faltantes
data_global = data_global[['new_cases']].asfreq('D').ffill()

# Mostrar las primeras filas para verificar
print(data_global.head())

# Ajustar el modelo Holt-Winters con tendencia y estacionalidad aditiva
hw_model = ExponentialSmoothing(
    data_global['new_cases'],
    trend='add',
    seasonal='add',
    seasonal_periods=7  # Suponemos estacionalidad semanal
).fit()

# Hacer predicciones para los próximos 30 días
hw_forecast = hw_model.forecast(steps=30)

# Mostrar las predicciones
print(hw_forecast)

hw_predictions = hw_model.forecast(steps=30)
plt.plot(hw_predictions.index, hw_predictions, label='Predicciones Holt-Winters')
plt.show()

from sklearn.metrics import mean_squared_error, mean_absolute_error

# Define el conjunto de prueba (por ejemplo, los últimos 30 días de datos reales)
test = data_global['new_cases'][-30:]  # Últimos 30 días reales

# Asegurarse de que los índices coinciden
predictions.index = test.index  # Alinear el índice de las predicciones con el de los datos de prueba

# Calcular MAD
mad = mean_absolute_error(test, predictions)
print(f"MAD: {mad}")

# Calcular MAPE (ajustado para evitar ceros)
def mape(y_true, y_pred):
    mask = y_true != 0  # Evitar divisiones por cero
    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100

mape_result = mape(test, predictions)
print(f"MAPE: {mape_result}")

# Calcular RMSE
rmse = np.sqrt(mean_squared_error(test, predictions))
print(f"RMSE: {rmse}")

"""# Comparación"""

from sklearn.metrics import mean_squared_error, mean_absolute_error
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.holtwinters import ExponentialSmoothing

# Divide los datos en entrenamiento y prueba (por ejemplo, últimos 30 días como prueba)
train = data_global['new_cases'][:-30]
test = data_global['new_cases'][-30:]

# ------------------------------------
# Ajustar el modelo ARIMA
arima_model = ARIMA(train, order=(1, 1, 1))
fitted_arima = arima_model.fit()

# Predicciones del modelo ARIMA
arima_predictions = fitted_arima.forecast(steps=len(test))
arima_predictions.index = test.index  # Alinear el índice

# ------------------------------------
# Ajustar el modelo Holt-Winters
hw_model = ExponentialSmoothing(train, trend='add', seasonal='add', seasonal_periods=7).fit()

# Predicciones del modelo Holt-Winters
hw_predictions = hw_model.forecast(steps=len(test))
hw_predictions.index = test.index  # Alinear el índice

# ------------------------------------
# Calcular MAD para ARIMA y Holt-Winters
mad_arima = mean_absolute_error(test, arima_predictions)
mad_hw = mean_absolute_error(test, hw_predictions)

print(f"MAD ARIMA: {mad_arima}")
print(f"MAD Holt-Winters: {mad_hw}")

# ------------------------------------
# Calcular MAPE para ARIMA y Holt-Winters
def mape(y_true, y_pred):
    mask = y_true != 0  # Evitar divisiones por cero
    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100

mape_arima = mape(test, arima_predictions)
mape_hw = mape(test, hw_predictions)

print(f"MAPE ARIMA: {mape_arima}")
print(f"MAPE Holt-Winters: {mape_hw}")

# ------------------------------------
# Calcular RMSE para ARIMA y Holt-Winters
rmse_arima = np.sqrt(mean_squared_error(test, arima_predictions))
rmse_hw = np.sqrt(mean_squared_error(test, hw_predictions))

print(f"RMSE ARIMA: {rmse_arima}")
print(f"RMSE Holt-Winters: {rmse_hw}")

# Eliminar duplicados en la columna de fecha, manteniendo solo una entrada por día
data_global = data[['date', 'new_cases']].copy()

# Convertir la columna 'date' a datetime y eliminar los duplicados, sumando los casos por fecha
data_global['date'] = pd.to_datetime(data_global['date'])
data_global = data_global.groupby('date').sum().reset_index()

# Asegurar frecuencia diaria y rellenar los valores faltantes
data_global.set_index('date', inplace=True)
data_global = data_global.asfreq('D').ffill()  # Rellenar los días faltantes con el valor anterior

# Verificar la estructura
print(data_global.head())

# Preparar los datos generales para todos los países, utilizando 'new_cases' de forma global
data_global = data[['date', 'new_cases']].copy()
data_global['date'] = pd.to_datetime(data_global['date'])
data_global = data_global.groupby('date').sum().reset_index()
data_global.set_index('date', inplace=True)
data_global = data_global.asfreq('D').ffill()  # Asegurar frecuencia diaria y rellenar valores faltantes

# Ajustar el modelo ARIMA en los datos globales
from statsmodels.tsa.arima.model import ARIMA
arima_model_global = ARIMA(data_global['new_cases'], order=(1, 1, 1))
fitted_arima_global = arima_model_global.fit()

# Ajustar el modelo Holt-Winters en los datos globales
from statsmodels.tsa.holtwinters import ExponentialSmoothing
hw_model_global = ExponentialSmoothing(
    data_global['new_cases'],
    trend='add',
    seasonal='add',
    seasonal_periods=7  # Asumimos estacionalidad semanal
).fit()

# ------------------------------------
# Predicciones para los próximos 30 días con ambos modelos
arima_predictions_global = fitted_arima_global.forecast(steps=30)
hw_predictions_global = hw_model_global.forecast(steps=30)

# Definir un conjunto de prueba con los últimos 30 días de datos reales
test_global = data_global['new_cases'][-30:]

# Alinear los índices de las predicciones con los valores reales
arima_predictions_global.index = test_global.index
hw_predictions_global.index = test_global.index

# ------------------------------------
# Calcular MAD, MAPE, RMSE para ARIMA
mad_arima_global = mean_absolute_error(test_global, arima_predictions_global)
rmse_arima_global = np.sqrt(mean_squared_error(test_global, arima_predictions_global))

def mape(y_true, y_pred):
    mask = y_true != 0  # Evitar divisiones por cero
    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100

mape_arima_global = mape(test_global, arima_predictions_global)

# ------------------------------------
# Calcular MAD, MAPE, RMSE para Holt-Winters
mad_hw_global = mean_absolute_error(test_global, hw_predictions_global)
rmse_hw_global = np.sqrt(mean_squared_error(test_global, hw_predictions_global))
mape_hw_global = mape(test_global, hw_predictions_global)


# Imprimir las primeras 5 predicciones para ambos modelos (ARIMA y Holt-Winters)
print("Predicciones ARIMA para los próximos 30 días:")
print(arima_predictions_global.head())  # Muestra las primeras 5 predicciones de ARIMA

print("\nPredicciones Holt-Winters para los próximos 30 días:")
print(hw_predictions_global.head())  # Muestra las primeras 5 predicciones de Holt-Winters

# Mostrar resultados
print(f"\nMAD ARIMA: {mad_arima_global}")
print(f"MAPE ARIMA: {mape_arima_global}")
print(f"RMSE ARIMA: {rmse_arima_global}")

print(f"\nMAD Holt-Winters: {mad_hw_global}")
print(f"MAPE Holt-Winters: {mape_hw_global}")
print(f"RMSE Holt-Winters: {rmse_hw_global}")

# Asumiendo que 'data_global' ya está preparada y es estacionaria
fig, ax = plt.subplots(2, 1, figsize=(10, 8))
fig = sm.graphics.tsa.plot_acf(data_global['new_cases'], lags=40, ax=ax[0])
fig = sm.graphics.tsa.plot_pacf(data_global['new_cases'], lags=40, ax=ax[1])
plt.show()

# Definir el conjunto de prueba (últimos 30 días de valores reales)
test_global = data_global['new_cases'][-30:]

# Asegurarse de que las predicciones tienen el mismo índice que los valores reales
arima_predictions_global.index = test_global.index
hw_predictions_global.index = test_global.index

# Crear el gráfico
plt.figure(figsize=(10, 6))

# Graficar los valores reales
plt.plot(test_global.index, test_global, label='Valores Reales', color='blue', marker='o')

# Graficar las predicciones de ARIMA
plt.plot(arima_predictions_global.index, arima_predictions_global, label='Predicciones ARIMA', color='green', linestyle='--', marker='x')

# Graficar las predicciones de Holt-Winters
plt.plot(hw_predictions_global.index, hw_predictions_global, label='Predicciones Holt-Winters', color='red', linestyle='--', marker='s')

# Añadir títulos y etiquetas
plt.title('Comparación de Valores Reales vs Modelos Predictivos (ARIMA y Holt-Winters)')
plt.xlabel('Fecha')
plt.ylabel('Nuevos Casos')
plt.legend()
plt.grid(True)

# Mostrar el gráfico
plt.show()